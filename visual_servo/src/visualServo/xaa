#!/usr/bin/env python
import rospy
import signal
from message_filters import Subscriber, TimeSynchronizer
from sensor_msgs.msg import Image, CameraInfo
from threading import Lock
import threading
import thread
import cv2
from cv_bridge import CvBridge
import helpers
import time
from objectDetector.objectDetector import objectDetector
import numpy as np
from matplotlib import pyplot as plt
from PIL import Image as imagePil
from cv2 import aruco 
import matplotlib  as mpl
import os
import tensorflow as tf
import math 

# Disable warnings tf #
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Change backend for plt #
plt.switch_backend('WXAgg')

# Read and handle frames #
class kinectHandler:

    def __init__(self):

        # Paths for detection model #
        self.modelPath = "/home/petropoulakis/Desktop/TensorFlow/workspace/robot_detection/model/frozen_inference_graph.pb"
        self.labelPath = "/home/petropoulakis/Desktop/TensorFlow/workspace/robot_detection/annotations/label_map.pbtxt"
        self.width = 960
        self.height = 540

        # Usefull vars #
        self.badPointsCount = 0 # Kinect failed
        self.badOrientation = 0 # PnPRansac failed
        self.lostFrames = 0 # Can not publish velocities
        self.maxLostTotalFPS = 10 # Max lost rate 
        self.maxLostConsecutiveFrames = 5 
        self.lostConsecutiveFrames = 0
        self.outOfBounds = False
        self.success = True

        # Servo parameters #
        self.kapa = 1.0 
        self.gama = 1.0  
        self.eStar = 1.2 # Desired error 

        # Set topic names #
        self.topicColor = "/kinect2/qhd/image_color_rect"
        self.topicDepth = "/kinect2/qhd/image_depth_rect"
        self.topicCameraInfoColor = "/kinect2/qhd/camera_info"
        self.topicCameraInfoDepth = "/kinect2/qhd/camera_info"

        # Frames #
        self.color = None
        self.depth = None

        # Camera matrix #
        self.cameraMatrixColor = None
        self.cameraMatrixDepth = None
        self.cameraDistortColor = None
        self.cameraDistortDepth = None

        self.errorCode = 0

        # Lock for messages #
        self.messagesMutex = Lock()
        self.newMessages = False

        # Listen color and depth # 
        self.subImageColor = Subscriber(self.topicColor, Image, queue_size=1)
        self.subImageDepth = Subscriber(self.topicDepth, Image, queue_size=1)
        self.subCameraInfoColor = Subscriber(self.topicCameraInfoColor, CameraInfo, queue_size=1)
        self.subCameraInfoDepth = Subscriber(self.topicCameraInfoDepth, CameraInfo, queue_size=1)

        # Sync messages #
        self.messages = TimeSynchronizer([self.subImageColor, self.subImageDepth, self.subCameraInfoColor, self.subCameraInfoDepth], queue_size=1)

        # Set callback function #
        self.messages.registerCallback(self.callback)

        # Initialize detector #
        self.pioneerDetector = objectDetector(self.modelPath, self.labelPath)

        # Set sig handler #
        signal.signal(signal.SIGINT, self.sigHandler)

    # Read camera messages #
    def callback(self, imageColor, imageDepth, cameraInfoColor, cameraInfoDepth):

        self.messagesMutex.acquire()
        
        # Camera Matrix #
        self.cameraMatrixColor, self.cameraDistortColor = helpers.readCameraInfo(cameraInfoColor)
        self.cameraMatrixDepth, self.cameraDistortDepth = helpers.readCameraInfo(cameraInfoDepth)
        
        # Frames #
        self.color = helpers.readImage(imageColor)
        self.depth = helpers.readImage(imageDepth)
        
        self.newMessages = True
        self.messagesMutex.release()

    # Detect object #
    def detect(self):
        frameCountFps = 0 # fps
        countReadCameraMatrix = 0
        #fpsText = ""
        #type = 0

        # Start listening messages #
        self.threadListener = threading.Thread(target=helpers.threadListenerFunc)
        self.threadListener.start()

        # Measure fps #
        startTimeFps = time.time()

        ##################
        # Process frames #
        ##################
        while(1):


            # Check state #
            if (self.lostFrames > self.maxLostTotalFPS or self.lostConsecutiveFrames > self.maxLostConsecutiveFrames):
                sel.success = False

                # Stop robot #
                self.stopRobot() 

                # Terminating #
                rospy.signal_shutdown("Closing kinect handler\n")

                # Wait thread #
                self.threadListener.join()
                break

            # Read current frame #
            self.messagesMutex.acquire()
            if(self.newMessages == True):
                currColor = self.color.copy()
                currDepth = self.depth.copy()
                self.newMessages = False
           
                # Read color camera matrix # 
                if(countReadCameraMatrix == 0):
                    K = self.cameraMatrixColor
                    D = self.cameraDistortColor
                    countReadCameraMatrix = 1
            else:
                self.messagesMutex.release()
                continue

            self.messagesMutex.release()

            # Measure fps #
            nowTimeFps = time.time()
            elapsed = nowTimeFps - startTimeFps
            if(elapsed >= 1):
                fps = frameCountFps / elapsed
                startTimeFps = nowTimeFps
                frameCount = 0
                #print(fps)

            frameCountFps += 1

            # Fix frame for detection #
            colorFixed = cv2.cvtColor(currColor, cv2.COLOR_BGR2RGB)
            colorExpand = np.expand_dims(colorFixed, axis=0)

            #type = 0
            # Detect #
            #result = self.pioneerDetector.predict(colorExpand)
            
            #xc, yc = self.pioneerDetector.getCenter(self.width, self.height, result["detection_boxes"][0])

            # Find angle for the center #
            #K = np.array([[self.cameraMatrixColor[0], self.cameraMatrixColor[1],self.cameraMatrixColor[2]],[self.cameraMatrixColor[3], self.cameraMatrixColor[4],self.cameraMatrixColor[5]], [self.cameraMatrixColor[6], self.cameraMatrixColor[7],self.cameraMatrixColor[8]]])
            #Ki = np.linalg.inv(K)
            #r1 = Ki.dot([self.width / 2.0 , self.height / 2.0, 1.0])       
            
            #r2 = Ki.dot([xc, yc, 1.0])       

            #cosAngle = r1.dot(r2) / (np.linalg.norm(r1) * np.linalg.norm(r2)) 
            #angleRadians = np.arccos(cosAngle)
           
            # Detect markers # 
            markers = aruco.Dictionary_get(aruco.DICT_6X6_250)
            params = aruco.DetectorParameters_create()
            corners, ids, rejectedPoints = aruco.detectMarkers(currColor, markers, parameters=params)
            frame_markers = aruco.drawDetectedMarkers(currColor.copy(), corners, ids) 
            #plt.figure()
            #plt.imshow(frame_markers)
            
            for i in range(len(ids)):
                c = corners[i][0]
                plt.plot([c[:, 0].mean()], [c[:, 1].mean()], 'o', label = "id={0}".format(ids[i]))
            #plt.legend()
            #plt.show()            
           
            mt = np.array([[1059.94655, 0.0, 954.8832], [0.0, 1053.93268, 523.73858],[0.0, 0.0, 0.0]])
            dist = np.asarray([[0.05626844], [-0.07419914], [0.001425079], [-0.00169517223], [0.02410768]])
            
            size_of_marker = 0.02
            rvecs, tvecs, trash = aruco.estimatePoseSingleMarkers(corners, size_of_marker, mt, dist)

            print(rvecs[-1], tvecs[-1])

            imaxis = aruco.drawDetectedMarkers(currColor.copy(), corners, ids)    
            for i in range(len(tvecs)):
                imaxis = aruco.drawAxis(imaxis, mt, dist, rvecs[i], tvecs[i], 0.05)
       
            #plt.imshow(frame_markers)
            #plt.show()
            # Real center of marker #
            c = corners[0][0]
            markerX = c[:, 0].mean() 
            markerY = c[:, 1].mean()
           
            xMean = int(markerX)
            yMean = (markerY)
            if checkOutOfBounds(xMean, yMean, self.width, self.height):
                self.outOfBounds = True    
                self.success = False
 
                # Stop robot #
                self.stopRobot() 

                # Terminating #
                rospy.signal_shutdown("Closing kinect handler\n")

                # Wait thread #
                self.threadListener.join()
                break

        



            # Find 3D coordinates for center #
            success, X, Y, Z = helpers.getXYZ(K, xMean, yMean, currDepth)
            if success == False:
                self.badPointsCount += 1
                continue

            print('x:' + str(X))
            print('Y:' + str(Y))
            print('z:' + str(Z))


            # Find orientation of object #
            success, rot, tran, inliers = helpers.findOrientation(K, D, xMean, yMean, depthMean)
            if success == False:
                self.badOrientation += 1
                continue

            print(self.servo(X, Y, Z, rot))
            raw_input("hi\n") 

            #for i in range (2):
            #    for j in range(7):
            #        currColor[int(markerY + i)][int(markerX + j)] = [0, 255, 0] 
            
            #cv2.imshow("image", currColor)
            #cv2.waitKey(0)
            #cv2.destroyAllWindows()

            #if sum(score >= 0.5 for score in result["detection_scores"]) > 1:
             #   type = 3

            #elif result["detection_scores"][0] < 0.65:
             #   if result["detection_scores"][0] < 0.5:
              #      type = 1
              # else:
              #    type = 2

            #print(type)
        
    # Find camera velocities with position based visual servo #
    # Publish coresponding robot velocities                   #  
    def servo(self, X, Y, Z, rot):
        e = math.sqrt(Z ** 2 + X ** 2) # Current error
        a = math.asin(Z / e) # Angle from goal
        if X < 0.0:
            a = math.pi - a
        
        uCamera = self.gama * math.cos(a) * (e - self.eStar) 
        omegaCamera = self.kapa * a + self.gama * math.cos(a) * math.sin(a)            
     
        return uCamera, omegaCamera 

        # Convert camera velocities to robot velocities #


        # Terminate condition #

            
        # Publish robot velocities #


    def stopRobot(self):
        pass

    # Handle signals for proper termination #
    def sigHandler(self, num, frame):
        rospy.signal_shutdown("Closing kinect handler\n")

        # Wait thread #
        self.threadListener.join()
        exit()

    def printOutcome(self):


        print("Success: " + str(self.success))


if __name__ == "__main__":
    rospy.init_node('kinect_handler')

    # Init #
    kinect =  kinectHandler()
    
    # Detect #
    kinect.detect()

# Petropoulakis Panagiotis
